{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Two Stage Sequential Classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Required Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, auc, confusion_matrix, f1_score, plot_confusion_matrix, precision_score, recall_score, roc_curve\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils.multiclass import unique_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## User Defined Functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None, display_labels=None, include_values=True, xticks_rotation='horizontal', values_format=None, cmap='viridis', ax=None, colorbar=True):\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, normalize=normalize)\n",
    "    \n",
    "    if display_labels is None:\n",
    "        if labels is None:\n",
    "            display_labels = unique_labels(y_true, y_pred)\n",
    "        else:\n",
    "            display_labels = labels\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                             display_labels=display_labels)\n",
    "    return disp.plot(include_values=include_values,\n",
    "                     cmap=cmap, ax=ax, xticks_rotation=xticks_rotation,\n",
    "                     values_format=values_format)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def specificity_score(y_true, y_pred):\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == 0 and y_true[i] == 0:\n",
    "            TN += 1\n",
    "        if y_pred[i] == 1 and y_true[i] == 0:\n",
    "            FP += 1\n",
    "    \n",
    "    return TN / (TN + FP)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def positive_pv_score(y_true, y_pred):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == y_pred[i] == 1:\n",
    "            TP += 1\n",
    "        elif y_true[i] == 0 and y_pred[i] == 1:\n",
    "            FP += 1\n",
    "\n",
    "    return TP/(TP+FP)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def negative_pv_score(y_true, y_pred):\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == y_pred[i] == 0:\n",
    "            TN += 1\n",
    "        elif y_true[i] == 1 and y_pred[i] == 0:\n",
    "            FN += 1\n",
    "    \n",
    "    return TN/(TN+FN) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_ci(bootstrapped_scores, name):\n",
    "    for i in range(3):\n",
    "        sorted_scores = np.array(bootstrapped_scores)[:, i]\n",
    "        sorted_scores.sort()\n",
    "    \n",
    "        confidence_lower = sorted_scores[int(0.025 * len(sorted_scores))]\n",
    "        confidence_upper = sorted_scores[int(0.975 * len(sorted_scores))]\n",
    "    \n",
    "        print(\"95% Confidence interval for the {} score for class {}: [{:0.4f} - {:0.4}]\".format(name, i,\n",
    "        confidence_lower, confidence_upper))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def my_classification_report(y_test_np, y_pred):\n",
    "\n",
    "    tot_TN = 0\n",
    "    tot_TP = 0\n",
    "    tot_FP = 0\n",
    "    tot_FN = 0\n",
    "\n",
    "    spec_arr = []\n",
    "    ppv_arr = []\n",
    "    npv_arr = []\n",
    "    rec_arr = []\n",
    "    acc_arr = []\n",
    "\n",
    "    print(\"Specificities\")\n",
    "    for group in range(3):\n",
    "        TN = 0\n",
    "        TP = 0\n",
    "        FN = 0\n",
    "        FP = 0\n",
    "        for i in range(len(y_test_np)):\n",
    "            if y_pred[i] != group and y_test_np[i] != group:\n",
    "                TN += 1\n",
    "            if y_pred[i] == group and y_test_np[i] != group:\n",
    "                FP += 1\n",
    "        tot_TN += TN\n",
    "        tot_FP += FP\n",
    "        spec_arr.append(TN/(TN+FP))\n",
    "        print(group, round(spec_arr[group],4))\n",
    "\n",
    "    print(\"Macro Avg : \" + str.format('{0:.4f}', np.array(spec_arr).mean()))\n",
    "    print(\"Micro Avg : \" + str.format('{0:.4f}', tot_TN / (tot_TN + tot_FP)), '\\n')\n",
    "\n",
    "    tot_TN = 0\n",
    "    tot_TP = 0\n",
    "    tot_FP = 0\n",
    "    tot_FN = 0\n",
    "\n",
    "    print(\"PPV/Precision\")\n",
    "    for group in range(3):\n",
    "        TN = 0\n",
    "        TP = 0\n",
    "        FN = 0\n",
    "        FP = 0\n",
    "        for i in range(len(y_test_np)):\n",
    "            if y_pred[i] == group and y_test_np[i] == group:\n",
    "                TP += 1\n",
    "            if y_pred[i] == group and y_test_np[i] != group:\n",
    "                FP += 1\n",
    "        tot_TP += TP\n",
    "        tot_FP += FP\n",
    "        ppv_arr.append(TP/(TP+FP))\n",
    "        print(group, round(ppv_arr[group],4))\n",
    "    \n",
    "    print(\"Macro Avg : \" + str.format('{0:.4f}', np.array(ppv_arr).mean()))\n",
    "    prec_for_mic_f1 = tot_TP / (tot_TP + tot_FP)\n",
    "    print(\"Micro Avg : \" + str.format('{0:.4f}', prec_for_mic_f1), '\\n')\n",
    "\n",
    "    tot_TN = 0\n",
    "    tot_TP = 0\n",
    "    tot_FP = 0\n",
    "    tot_FN = 0\n",
    "\n",
    "    print(\"NPV\")\n",
    "    for group in range(3):\n",
    "        TN = 0\n",
    "        TP = 0\n",
    "        FN = 0\n",
    "        FP = 0\n",
    "        for i in range(len(y_test_np)):\n",
    "            if y_pred[i] != group and y_test_np[i] != group:\n",
    "                TN += 1\n",
    "            if y_pred[i] != group and y_test_np[i] == group:\n",
    "                FN += 1\n",
    "        tot_TN += TN\n",
    "        tot_FN += FN\n",
    "        npv_arr.append(TN/(TN+FN))\n",
    "        print(group, round(npv_arr[group],4))\n",
    "    \n",
    "    print(\"Macro Avg : \" + str.format('{0:.4f}', np.array(npv_arr).mean()))\n",
    "    print(\"Micro Avg : \" + str.format('{0:.4f}', tot_TN / (tot_TN + tot_FN)), '\\n') \n",
    "    \n",
    "    tot_TN = 0\n",
    "    tot_TP = 0\n",
    "    tot_FP = 0\n",
    "    tot_FN = 0\n",
    "\n",
    "    print(\"Recall\")\n",
    "    for group in range(3):\n",
    "        TN = 0\n",
    "        TP = 0\n",
    "        FN = 0\n",
    "        FP = 0\n",
    "        for i in range(len(y_test_np)):\n",
    "            if y_pred[i] == group and y_test_np[i] == group:\n",
    "                TP += 1\n",
    "            if y_pred[i] != group and y_test_np[i] == group:\n",
    "                FN += 1\n",
    "        tot_TP += TP\n",
    "        tot_FN += FN\n",
    "        rec_arr.append(TP/(TP+FN))\n",
    "        print(group, round(rec_arr[group],4))\n",
    "    \n",
    "    print(\"Macro Avg : \" + str.format('{0:.4f}', np.array(rec_arr).mean()))\n",
    "    rec_for_mic_f1 = tot_TP / (tot_TP + tot_FN)\n",
    "    print(\"Micro Avg : \" + str.format('{0:.4f}', rec_for_mic_f1), '\\n')\n",
    "    \n",
    "    print(\"F1 Score\")\n",
    "    for group in range(3):\n",
    "        if rec_arr[group] + ppv_arr[group] == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = 2 * (rec_arr[group] * ppv_arr[group]) / (rec_arr[group] + ppv_arr[group])\n",
    "        print(group, round(f1, 4))\n",
    "    print(\"Macro Avg : \" + str.format('{0:.4f}', 2 * (np.array(rec_arr).mean() * np.array(ppv_arr).mean()) \n",
    "                               / (np.array(rec_arr).mean() + np.array(ppv_arr).mean())))\n",
    "    print(\"Micro Avg : \" + str.format('{0:.4f}', 2 * (prec_for_mic_f1 * rec_for_mic_f1)/ (prec_for_mic_f1 + rec_for_mic_f1)), '\\n')\n",
    "    \n",
    "    tot_TN = 0\n",
    "    tot_TP = 0\n",
    "    tot_FP = 0\n",
    "    tot_FN = 0   \n",
    "    \n",
    "    print(\"Accuracy:\")\n",
    "    for group in range(3):\n",
    "        TN = 0\n",
    "        TP = 0\n",
    "        FN = 0\n",
    "        FP = 0\n",
    "        for i in range(len(y_test_np)):\n",
    "            if y_pred[i] == group and y_test_np[i] == group:\n",
    "                TP += 1\n",
    "            if y_pred[i] != group and y_test_np[i] != group:\n",
    "                TN += 1\n",
    "            if y_pred[i] == group and y_test_np[i] != group:\n",
    "                FP += 1\n",
    "            if y_pred[i] != group and y_test_np[i] == group:\n",
    "                FN += 1\n",
    "        \n",
    "        tot_TP += TP\n",
    "        tot_TN += TN\n",
    "        tot_FP += FP\n",
    "        tot_FN += FN\n",
    "        \n",
    "        acc_arr.append((TP + TN)/(TP + TN + FP + FN))\n",
    "        print(group, round(acc_arr[group],10))\n",
    "\n",
    "    print(\"Macro Avg : \" + str.format('{0:.4f}', np.array(acc_arr).mean()))\n",
    "    print(\"Micro Avg : \" + str.format('{0:.4f}', (tot_TP + tot_TN)/(tot_TP + tot_TN + tot_FP + tot_FN)), '\\n') ##\n",
    "\n",
    "    print('multiclass acc:', (y_test_np == y_pred).sum() / len(y_test_np))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_metrics(y_test, y_pred):\n",
    "    print(\"Sens Score: \" + str.format('{0:.4f}', (recall_score(y_test, y_pred))))\n",
    "    print(\"Spec Score: \" + str.format('{0:.4f}', (specificity_score(y_test.to_numpy(), y_pred))))\n",
    "    print(\"PPV Score: \" + str.format('{0:.4f}', (positive_pv_score(y_test.to_numpy(), y_pred))))\n",
    "    print(\"NPV Score: \" + str.format('{0:.4f}', (negative_pv_score(y_test.to_numpy(), y_pred))))\n",
    "    print(\"Acc Score: \" + str.format('{0:.4f}', (accuracy_score(y_test, y_pred))))\n",
    "    print(\"F1 Score: \" + str.format('{0:.4f}', (f1_score(y_test, y_pred))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_roc_curve(model_name):\n",
    "    sns.set()\n",
    "    plt.figure(figsize=(7, 7))\n",
    "\n",
    "    ns_preds = [0 for _ in range(len(y_test))]\n",
    "\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_preds)\n",
    "\n",
    "    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "    plt.plot(fpr, tpr, marker='.', label=model_name)\n",
    "    plt.xlabel('1 - Specificity (False Positive Rate)',fontsize=16)\n",
    "    plt.ylabel('Sensitivity (True Positive Rate)',fontsize=16)\n",
    "\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title(f'{model_name}: ROC Curve for Test Set', fontsize=20, fontweight=\"semibold\")\n",
    "    short_auc = round(auc,4)\n",
    "    plt.text(.93,.1, \"AUC: \" + str(short_auc), \n",
    "        horizontalalignment=\"center\", verticalalignment=\"center\",\n",
    "        fontsize=14, fontweight=\"semibold\")\n",
    "    \n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_intermed_confusion_matrix(model_name, model, X, y, normalize=None):\n",
    "    matrix = plot_confusion_matrix(model, X, y,\n",
    "                               cmap=plt.cm.Blues,\n",
    "                                  normalize=normalize)\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.show(matrix)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def grid_search(model, params, X_train, y_train):\n",
    "    grid = GridSearchCV(estimator=model,\n",
    "                       param_grid=params,\n",
    "                       scoring='accuracy',\n",
    "                       cv=5,\n",
    "                       n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    return grid.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loading"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv(\"features-n.csv\")\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# obtain list of features selected from LASSO\n",
    "reduced_features_list = ['t1_log-sigma-1-mm-3D_glszm_SmallAreaLowGrayLevelEmphasis', 't1_log-sigma-3-mm-3D_firstorder_Median', 't1_log-sigma-3-mm-3D_glcm_ClusterProminence', 't1_log-sigma-3-mm-3D_glrlm_LowGrayLevelRunEmphasis', 't1_log-sigma-5-mm-3D_glszm_GrayLevelVariance', 't1_original_glszm_LargeAreaHighGrayLevelEmphasis', 't1_original_glszm_LargeAreaLowGrayLevelEmphasis', 't1_original_shape_Flatness', 't1_wavelet-HHL_firstorder_Skewness', 't1_wavelet-HLL_firstorder_Mean', 't1_wavelet-HLL_glcm_Correlation', 't1_wavelet-LHL_glcm_Correlation', 't1_wavelet-LLH_glszm_LargeAreaLowGrayLevelEmphasis', 't1_wavelet-LLL_firstorder_Skewness', 't1_wavelet-LLL_glszm_ZoneVariance', 't2_log-sigma-1-mm-3D_firstorder_Mean', 't2_log-sigma-1-mm-3D_firstorder_Median', 't2_log-sigma-1-mm-3D_glcm_Correlation', 't2_log-sigma-5-mm-3D_firstorder_Kurtosis', 't2_log-sigma-5-mm-3D_glrlm_RunLengthNonUniformity', 't2_log-sigma-5-mm-3D_glszm_SmallAreaEmphasis', 't2_original_glszm_GrayLevelNonUniformity', 't2_wavelet-HHH_glrlm_RunPercentage', 't2_wavelet-HHH_glszm_LargeAreaEmphasis', 't2_wavelet-HHH_glszm_SmallAreaLowGrayLevelEmphasis', 't2_wavelet-HHL_glcm_ClusterProminence', 't2_wavelet-HHL_glrlm_LongRunHighGrayLevelEmphasis', 't2_wavelet-HHL_glszm_LargeAreaHighGrayLevelEmphasis', 't2_wavelet-HLH_glszm_SizeZoneNonUniformityNormalized', 't2_wavelet-HLL_firstorder_Mean', 't2_wavelet-LHH_firstorder_Mean', 't2_wavelet-LHH_glszm_GrayLevelNonUniformityNormalized', 't2_wavelet-LHL_glcm_Idn', 't2_wavelet-LLL_firstorder_Energy', 't2_wavelet-LLL_glcm_JointAverage', 't1_log-sigma-3-mm-3D_glrlm_ShortRunLowGrayLevelEmphasis', 't1_wavelet-HHH_firstorder_Skewness', 't1_wavelet-HHH_glszm_LargeAreaHighGrayLevelEmphasis', 't1_wavelet-HHL_glcm_ClusterProminence', 't1_wavelet-LHH_firstorder_Median', 't2_log-sigma-5-mm-3D_glcm_Imc2', 't2_original_glcm_Idmn', 't2_wavelet-LLL_firstorder_Median', 't2_wavelet-HHH_firstorder_Skewness', 't2_original_firstorder_Kurtosis', 't2_wavelet-LLL_firstorder_TotalEnergy']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NN1: group3+4 vs rest (First Sequential)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "first_features = df[reduced_features_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Binarizing target\n",
    "first_target = df[\"molecular\"]\n",
    "\n",
    "# Marking 0 as group3+4 and 1 as wnt\n",
    "first_target = first_target.map(dict(group3=0, group4=0, shh=1, wnt=1))\n",
    "print(first_target.value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "train_idx = df['split'] != 'test'\n",
    "test_idx = ~train_idx\n",
    "# X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(first_features, first_target, test_size = 0.25, random_state = 42)\n",
    "X_train_1 = first_features[train_idx]\n",
    "y_train_1 = first_target[train_idx]\n",
    "X_test_1 = first_features[test_idx]\n",
    "y_test_1 = first_target[test_idx]\n",
    "print(\"TRAIN\")\n",
    "print(y_train_1.value_counts())\n",
    "\n",
    "print(\"TEST\")\n",
    "print(y_test_1.value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #NIR\n",
    "# NIR_G34 = 42/66\n",
    "# NIR_SHH = 14/66\n",
    "# NIR_WNT = 10/66\n",
    "#\n",
    "# print('%.4f' % NIR_G34)\n",
    "# print('%.4f' % NIR_SHH)\n",
    "# print('%.4f' % NIR_WNT)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Resampling to Correct for Imbalance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "first_X = pd.concat([X_train_1, y_train_1], axis = 1)\n",
    "first_X_0 = first_X[first_X['molecular'] == 0]\n",
    "first_X_1 = first_X[first_X['molecular'] == 1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "assert len(first_X_0) >= len(first_X_1)\n",
    "upsampled_1 = resample(first_X_1, replace = True, n_samples = len(first_X_0), random_state = 42)\n",
    "upsampled = pd.concat([upsampled_1, first_X_0])\n",
    "upsampled = upsampled.sample(frac = 1, random_state = 42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_1 = upsampled.iloc[:, :-1]\n",
    "y_train_1 = upsampled.iloc[:, -1]\n",
    "print(y_train_1.value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Standardizing Features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "first_names = X_train_1.columns\n",
    "first_scaler = preprocessing.StandardScaler()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_1 = first_scaler.fit_transform(X_train_1)\n",
    "X_train_1 = pd.DataFrame(X_train_1, columns = first_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_1 = first_scaler.transform(X_test_1)\n",
    "X_test_1 = pd.DataFrame(X_test_1, columns = first_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Grid Search for Hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nn_model_1_model = MLPClassifier(max_iter = 2000, random_state = 42)\n",
    "nn_grid_params_1 = {'hidden_layer_sizes': [(100, 100, 50), (50, 100, 50), (100, 50, 100)],\n",
    "              'learning_rate': ['constant', 'invscaling', 'adaptive']\n",
    "}\n",
    "\n",
    "nn_params_1 = grid_search(nn_model_1_model, nn_grid_params_1, X_train_1, y_train_1)\n",
    "print(nn_params_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model Performance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nn_seq_1_model = MLPClassifier(nn_params_1['hidden_layer_sizes'], max_iter=2000, random_state=42, learning_rate=nn_params_1['learning_rate'])\n",
    "nn_seq_1_model.fit(X_train_1, y_train_1)\n",
    "y_pred_1 = nn_seq_1_model.predict(X_test_1)\n",
    "print(X_test_1.shape)\n",
    "print(y_pred_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_metrics(y_test_1, y_pred_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NN2: SHH vs WNT (Second Sequential) [decomposed from rest1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# obtain list of features selected from LASSO\n",
    "second_reduced_features_list = ['t1_log-sigma-3-mm-3D_glszm_GrayLevelNonUniformityNormalized', 't1_wavelet-LHL_glcm_Correlation', 't1_wavelet-LHL_glcm_Idn', 't2_wavelet-HHL_firstorder_InterquartileRange', 't2_wavelet-LLL_firstorder_Kurtosis', 't2_log-sigma-1-mm-3D_glszm_HighGrayLevelZoneEmphasis']\n",
    "print(len(second_reduced_features_list))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# second_features = df.drop(['molecular','molec_id', 'dx_date','alive','os','pfs','seg_id'], axis = 1)\n",
    "# second_features = second_features[second_reduced_features_list]\n",
    "second_df = df[df['molecular'].isin(['wnt', 'shh'])]\n",
    "second_features = second_df[second_reduced_features_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Binarizing target\n",
    "second_target = second_df[\"molecular\"]\n",
    "\n",
    "# Marking 0 as shh and 1 as wnt\n",
    "second_target = second_target.map({\n",
    "    'shh': 0,\n",
    "    'wnt': 1,\n",
    "})\n",
    "print(second_target.value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# second_target = second_target.reset_index(drop = True)\n",
    "# second_features = second_features.reset_index(drop = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train_2 = second_features[train_idx]\n",
    "X_test_2 = second_features[test_idx]\n",
    "y_train_2 = second_target[train_idx]\n",
    "y_test_2 = second_target[test_idx]\n",
    "# X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(second_features, second_target,\n",
    "#                                                     test_size = 0.25, random_state = 42)\n",
    "print(\"TRAIN\")\n",
    "print(y_train_2.value_counts())\n",
    "\n",
    "print(\"TEST\")\n",
    "print(y_test_2.value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Resampling to Correct for Imbalance "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "second_X = pd.concat([X_train_2, y_train_2], axis = 1)\n",
    "second_X_0 = second_X[second_X['molecular'] == 0]\n",
    "second_X_1 = second_X[second_X['molecular'] == 1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "assert len(second_X_0) >= len(second_X_1)\n",
    "second_upsampled_1 = resample(second_X_1, replace = True, n_samples = len(second_X_0), random_state = 42)\n",
    "second_upsampled = pd.concat([second_upsampled_1, second_X_0])\n",
    "second_upsampled = second_upsampled.sample(frac = 1, random_state = 42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_2 = second_upsampled.iloc[:, :-1]\n",
    "y_train_2 = second_upsampled.iloc[:, -1]\n",
    "print(y_train_2.value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Standardizing Features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "second_names = X_train_2.columns\n",
    "second_scaler = preprocessing.StandardScaler()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_2 = second_scaler.fit_transform(X_train_2)\n",
    "X_train_2 = pd.DataFrame(X_train_2, columns = second_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_2 = second_scaler.transform(X_test_2)\n",
    "X_test_2 = pd.DataFrame(X_test_2, columns = second_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modeling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model Performance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nn_model_2_model = MLPClassifier(max_iter=2000, random_state=42)\n",
    "nn_grid_params_2 = {'hidden_layer_sizes': [(100, 100, 50), (50, 100, 50), (100, 50, 100)],\n",
    "              'learning_rate': ['constant', 'invscaling', 'adaptive']\n",
    "}\n",
    "\n",
    "nn_params_2 = grid_search(nn_model_2_model, nn_grid_params_2, X_train_2, y_train_2)\n",
    "print(nn_params_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nn_seq_2_model = MLPClassifier(nn_params_2['hidden_layer_sizes'], max_iter=2000, random_state=42, learning_rate=nn_params_2['learning_rate'])\n",
    "nn_seq_2_model.fit(X_train_2, y_train_2)\n",
    "y_pred_2 = nn_seq_2_model.predict(X_test_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "create_intermed_confusion_matrix('Neural Network', nn_seq_2_model, X_test_2, y_test_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test features\n",
    "# test_features = df.drop(['molecular','molec_id', 'dx_date','alive','os','pfs','seg_id'], axis = 1)\n",
    "test_features = df.copy()\n",
    "test_target = df['molecular']\n",
    "# test_target = test_target.map(dict(group3 = 0, shh = 1, wnt = 2))\n",
    "test_target = test_target.map({\n",
    "    'group3': 0,\n",
    "    'group4': 0,\n",
    "    'shh': 1,\n",
    "    'wnt': 2,\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(test_features, test_target,\n",
    "#                                                     test_size = 0.25, random_state = 42)\n",
    "X_train = test_features[train_idx]\n",
    "X_test = test_features[test_idx]\n",
    "y_train = test_target[train_idx]\n",
    "y_test = test_target[test_idx]\n",
    "print(\"TRAIN\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"TEST\")\n",
    "print(y_test.value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "# X_test = X_test.reset_index(drop = True)\n",
    "# y_test = y_test.reset_index(drop = True)\n",
    "# sex_binarized = X_test['sex'].map(dict(M = 1, F = 0)).to_numpy()\n",
    "# X_test['sex'] = sex_binarized\n",
    "X_test_reduced = X_test[reduced_features_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Standardization for NN1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "names = X_test_reduced.columns\n",
    "X_test_for_first = first_scaler.transform(X_test_reduced)\n",
    "X_test_for_first = pd.DataFrame(X_test_for_first, columns = names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modeling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_preds_after_first = nn_seq_1_model.predict(X_test_for_first)\n",
    "print(y_preds_after_first)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#keeps all that are group3+4, indifferent to shh/wnt label\n",
    "group3_indices = np.where(y_preds_after_first == 0)\n",
    "other_indices = np.where(y_preds_after_first != 0)\n",
    "\n",
    "group3_preds = y_preds_after_first[group3_indices]\n",
    "y_test_for_group3 = np.array(y_test)[group3_indices]\n",
    "\n",
    "X_test_after_first_model = X_test.iloc[other_indices].reset_index(drop = True)\n",
    "y_test_after_first_model = y_test.iloc[other_indices].reset_index(drop = True)\n",
    "\n",
    "X_test_after_first_model = X_test_after_first_model[second_reduced_features_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Standardization for NN2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_after_first_model = second_scaler.transform(X_test_after_first_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#relabel original test-dictionary\n",
    "#allows us to dro group3 from 2nd stage\n",
    "#allows accuracy_score of binary outputs: shh now 0, wnt now 1\n",
    "y_test_after_first_model = np.array(y_test_after_first_model)\n",
    "y_test_after_first_model[y_test_after_first_model == 0] = -1\n",
    "y_test_after_first_model[y_test_after_first_model == 1] = 0\n",
    "y_test_after_first_model[y_test_after_first_model == 2] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_preds_after_second = nn_seq_2_model.predict(X_test_after_first_model)\n",
    "print(y_preds_after_second)\n",
    "print(accuracy_score(y_test_after_first_model, y_preds_after_second))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Post-processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#relabel prediction outputs to match 3-way; wnt to 2, shh to 1\n",
    "#group3 is 0 already from \"group3_preds\" and \"y_test_for_group3\" derived from \"group3_indices\"\n",
    "y_preds_after_second[y_preds_after_second == 1] = 2\n",
    "y_preds_after_second[y_preds_after_second == 0] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#revert test-labels to 3-way; wnt back to 2, shh back to 1, group3 back to 0\n",
    "y_test_after_first_model[y_test_after_first_model == 1] = 2\n",
    "y_test_after_first_model[y_test_after_first_model == 0] = 1\n",
    "y_test_after_first_model[y_test_after_first_model == -1] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_preds_overall = np.concatenate([y_preds_after_second, group3_preds])\n",
    "y_test_overall = np.concatenate([y_test_after_first_model, y_test_for_group3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Acc: \" + str(accuracy_score(y_test_overall, y_preds_overall)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "matrix = create_confusion_matrix(y_test_overall, y_preds_overall, cmap=plt.cm.Blues)\n",
    "plt.title('NN1 + NN2 Confusion Matrix')\n",
    "plt.show(matrix)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "matrix = create_confusion_matrix(y_test_overall, y_preds_overall, cmap=plt.cm.Blues, normalize = 'true')\n",
    "plt.title('Normalized NN1 + NN2 Confusion Matrix')\n",
    "plt.show(matrix)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3-way scores\n",
    "y_test_np = y_test.to_numpy()\n",
    "my_classification_report(y_test_overall, y_preds_overall)\n",
    "\n",
    "y_test_bin = label_binarize(y_test_overall, classes = [0, 1, 2])\n",
    "y_pred_bin = label_binarize(y_preds_overall, classes = [0, 1, 2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Confidence Intervals"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "\n",
    "n_bootstraps = 2000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_auc_scores = []\n",
    "bootstrapped_acc_scores = []\n",
    "bootstrapped_spec_scores = []\n",
    "bootstrapped_prec_scores = []\n",
    "bootstrapped_rec_scores = []\n",
    "bootstrapped_f_one_scores = []\n",
    "bootstrapped_ppv_scores = [] ##\n",
    "bootstrapped_npv_scores = [] ##\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(y_test_overall), len(y_test_overall))\n",
    "    if len(np.unique(y_test_overall[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "    \n",
    "    one_acc = []\n",
    "    one_spec = []\n",
    "    one_prec = []\n",
    "    one_rec = []\n",
    "    one_f_one = []\n",
    "    one_ppv = [] ##\n",
    "    one_npv = [] ##\n",
    "    for i in range(n_classes):\n",
    "        one_acc.append(accuracy_score(y_test_bin[:, i][indices], y_pred_bin[:, i][indices]))\n",
    "        one_spec.append(specificity_score(y_test_bin[:, i][indices], y_pred_bin[:, i][indices]))\n",
    "        one_prec.append(precision_score(y_test_bin[:, i][indices], y_pred_bin[:, i][indices]))\n",
    "        one_rec.append(recall_score(y_test_bin[:, i][indices], y_pred_bin[:, i][indices]))\n",
    "        one_f_one.append(f1_score(y_test_bin[:, i][indices], y_pred_bin[:, i][indices]))\n",
    "        one_ppv.append(positive_pv_score(y_test_bin[:, i][indices], y_pred_bin[:, i][indices])) ##\n",
    "        one_npv.append(negative_pv_score(y_test_bin[:, i][indices], y_pred_bin[:, i][indices])) ##\n",
    "\n",
    "    bootstrapped_acc_scores.append(one_acc)\n",
    "    bootstrapped_spec_scores.append(one_spec)\n",
    "    bootstrapped_prec_scores.append(one_prec)\n",
    "    bootstrapped_rec_scores.append(one_rec)\n",
    "    bootstrapped_f_one_scores.append(one_f_one)\n",
    "    bootstrapped_ppv_scores.append(one_ppv) ##\n",
    "    bootstrapped_npv_scores.append(one_npv) ##"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "create_ci(bootstrapped_acc_scores, \"Accuracy\")\n",
    "create_ci(bootstrapped_spec_scores, \"Specificity\")\n",
    "create_ci(bootstrapped_prec_scores, \"Precision\")\n",
    "create_ci(bootstrapped_rec_scores, \"Recall\")\n",
    "create_ci(bootstrapped_f_one_scores, \"F1\")\n",
    "create_ci(bootstrapped_ppv_scores, \"PPV\") ##\n",
    "create_ci(bootstrapped_npv_scores, \"NPV\") ##"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Individual Classification Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metadata_df = df['molecular']\n",
    "target_for_table = df['molecular']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# features_for_table = df.drop(['molecular','molec_id', 'dx_date','alive','os','pfs','seg_id'], axis = 1)\n",
    "features_for_table = df.drop(columns='molecular')\n",
    "# target_for_table = target_for_table.map(dict(group3=0, shh = 1, wnt = 2))\n",
    "target_for_table = target_for_table.map({\n",
    "    'group3': 0,\n",
    "    'group4': 0,\n",
    "    'shh': 1,\n",
    "    'wnt': 2,\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#only used for metadata for which is train and which is test\n",
    "# X_train_for_table, X_test_for_table, _, _ = train_test_split(features_for_table, target_for_table,\n",
    "#                                                     test_size = 0.25, random_state = 42)\n",
    "X_train_for_table = features_for_table[train_idx]\n",
    "X_test_for_table = features_for_table[test_idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_for_table['Set'] = 'training'\n",
    "X_test_for_table['Set'] = 'test'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_for_table = pd.concat([X_train_for_table, X_test_for_table])\n",
    "X_for_table = pd.merge(metadata_df, X_for_table, left_index = True, right_index = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_df = X_for_table[['molecular', 'Set']]\n",
    "# features_for_table = df.drop(['molecular','molec_id', 'dx_date','alive','os','pfs','seg_id'], axis = 1)\n",
    "features_for_table = df.drop(columns='molecular')\n",
    "target_for_table = X_for_table['molecular']\n",
    "target_for_table = target_for_table.map(dict(group3 = 0, shh = 1, wnt = 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_table_reduced = features_for_table[reduced_features_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sex_binarized = X_table_reduced['sex'].map(dict(M = 1, F = 0)).to_numpy()\n",
    "#\n",
    "# X_table_reduced['sex'] = sex_binarized"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "names = X_table_reduced.columns\n",
    "X_table_for_first = first_scaler.transform(X_table_reduced)\n",
    "X_table_for_first = pd.DataFrame(X_table_for_first, columns = names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_probs_after_first_table = nn_seq_1_model.predict_proba(X_table_for_first)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_1_table = pd.DataFrame(y_probs_after_first_table, columns = ['NN1 Group3 Prob','NN1 Non-Group3 Prob'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_df['NN1 Group3 Prob'] = rf_1_table['NN1 Group3 Prob']\n",
    "final_df['NN1 Non-Group3 Prob'] = rf_1_table['NN1 Non-Group3 Prob']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_preds_after_first_table = nn_seq_1_model.predict(X_table_for_first)\n",
    "wnt_table_indices = np.where(y_preds_after_first_table == 0)\n",
    "other_table_indices = np.where(y_preds_after_first_table != 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_table_for_wnt = np.array(y_preds_after_first_table)[wnt_table_indices]\n",
    "X_table_after_first_model = features_for_table.iloc[other_table_indices]\n",
    "X_table_after_first_model = X_table_after_first_model[second_reduced_features_list]\n",
    "X_table_after_first_model_arr = second_scaler.transform(X_table_after_first_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_probs_after_second_table = nn_seq_2_model.predict_proba(X_table_after_first_model_arr)\n",
    "y_preds_after_second_table = nn_seq_2_model.predict(X_table_after_first_model_arr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nn_2_table = pd.DataFrame(y_probs_after_second_table, columns = ['NN2 SHH Prob','NN2 WNT Prob'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_table_after_first_model['NN2 SHH Prob'] = np.array(nn_2_table['NN2 SHH Prob'])\n",
    "X_table_after_first_model['NN2 WNT Prob'] = np.array(nn_2_table['NN2 WNT Prob'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_table_after_first_model = X_table_after_first_model[['NN2 SHH Prob', 'NN2 WNT Prob']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_df = final_df.merge(X_table_after_first_model, how='left', left_index=True, right_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def f(row):\n",
    "    if row['NN1 Non-Group3 Prob'] < row['NN1 Group3 Prob']:\n",
    "        val = 'group3'\n",
    "    else:\n",
    "        if row['NN2 SHH Prob'] > row['NN2 WNT Prob']:\n",
    "            val = 'shh'\n",
    "        else:\n",
    "            val = 'wnt'\n",
    "    return val"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# final_df['molec_id'] = df['molec_id']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_df['Pred Path'] = final_df.apply(f, axis = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Formatting\n",
    "# final_df.columns = ['True Molec', 'Set', 'NN1 Non-Group3 Prob', 'NN1 Group3 Prob', 'NN2 SHH Prob', 'NN2 WNT Prob', 'Molec Id', 'Pred Molec']\n",
    "final_df.columns = ['True Molec', 'Set', 'NN1 Non-Group3 Prob', 'NN1 Group3 Prob', 'NN2 SHH Prob', 'NN2 WNT Prob', 'Pred Molec']\n",
    "cols = final_df.columns.tolist()\n",
    "cols = ['NN1 Non-Group3 Prob', 'NN1 Group3 Prob', 'NN2 SHH Prob', 'NN2 WNT Prob',\n",
    "        'Pred Molec', 'True Molec', 'Set']\n",
    "final_df = final_df[cols]\n",
    "final_df = final_df.round(4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Formatting\n",
    "# final_df.columns = ['True Molec', 'Set', 'NN1 Non-Group3 Prob', 'NN1 Group3 Prob', 'NN2 SHH Prob', 'NN2 WNT Prob', 'Molec Id', 'Pred Molec']\n",
    "final_df.columns = ['True Molec', 'Set', 'NN1 Non-Group3 Prob', 'NN1 Group3 Prob', 'NN2 SHH Prob', 'NN2 WNT Prob', 'Pred Molec']\n",
    "cols = final_df.columns.tolist()\n",
    "cols = ['NN1 Non-Group3 Prob', 'NN1 Group3 Prob', 'NN2 SHH Prob', 'NN2 WNT Prob',\n",
    "        'Pred Molec', 'True Molec', 'Set']\n",
    "final_df = final_df[cols]\n",
    "final_df = final_df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     NN1 Non-Group3 Prob  NN1 Group3 Prob  NN2 SHH Prob  NN2 WNT Prob  \\\n",
      "0                 1.0000           0.0000           NaN           NaN   \n",
      "1                 0.9905           0.0095           NaN           NaN   \n",
      "2                 0.9984           0.0016           NaN           NaN   \n",
      "3                 0.9989           0.0011           NaN           NaN   \n",
      "4                 0.7932           0.2068           NaN           NaN   \n",
      "5                 1.0000           0.0000           NaN           NaN   \n",
      "6                 0.9817           0.0183           NaN           NaN   \n",
      "7                 0.9971           0.0029           NaN           NaN   \n",
      "8                 0.0024           0.9976        0.7802        0.2198   \n",
      "9                 0.9939           0.0061           NaN           NaN   \n",
      "10                0.9997           0.0003           NaN           NaN   \n",
      "11                0.9970           0.0030           NaN           NaN   \n",
      "12                0.9990           0.0010           NaN           NaN   \n",
      "13                0.9871           0.0129           NaN           NaN   \n",
      "14                0.9997           0.0003           NaN           NaN   \n",
      "15                0.9987           0.0013           NaN           NaN   \n",
      "16                1.0000           0.0000           NaN           NaN   \n",
      "17                1.0000           0.0000           NaN           NaN   \n",
      "18                0.9733           0.0267           NaN           NaN   \n",
      "19                0.7670           0.2330           NaN           NaN   \n",
      "20                0.5953           0.4047           NaN           NaN   \n",
      "21                0.0089           0.9911        0.9851        0.0149   \n",
      "22                0.9996           0.0004           NaN           NaN   \n",
      "23                0.0061           0.9939        1.0000        0.0000   \n",
      "24                0.9991           0.0009           NaN           NaN   \n",
      "25                1.0000           0.0000           NaN           NaN   \n",
      "26                1.0000           0.0000           NaN           NaN   \n",
      "27                0.3533           0.6467        0.8183        0.1817   \n",
      "28                0.9999           0.0001           NaN           NaN   \n",
      "29                0.0106           0.9894        0.0011        0.9989   \n",
      "30                0.9219           0.0781           NaN           NaN   \n",
      "31                0.2211           0.7789        0.0005        0.9995   \n",
      "32                0.9220           0.0780           NaN           NaN   \n",
      "33                1.0000           0.0000           NaN           NaN   \n",
      "34                0.9851           0.0149           NaN           NaN   \n",
      "35                1.0000           0.0000           NaN           NaN   \n",
      "36                0.7495           0.2505           NaN           NaN   \n",
      "37                0.9987           0.0013           NaN           NaN   \n",
      "38                0.5435           0.4565           NaN           NaN   \n",
      "39                1.0000           0.0000           NaN           NaN   \n",
      "40                0.9992           0.0008           NaN           NaN   \n",
      "41                0.9998           0.0002           NaN           NaN   \n",
      "42                1.0000           0.0000           NaN           NaN   \n",
      "43                0.0004           0.9996        1.0000        0.0000   \n",
      "44                0.9227           0.0773           NaN           NaN   \n",
      "45                0.9991           0.0009           NaN           NaN   \n",
      "46                0.0040           0.9960        1.0000        0.0000   \n",
      "47                0.0006           0.9994        1.0000        0.0000   \n",
      "48                0.9982           0.0018           NaN           NaN   \n",
      "49                0.9962           0.0038           NaN           NaN   \n",
      "50                1.0000           0.0000           NaN           NaN   \n",
      "51                0.9999           0.0001           NaN           NaN   \n",
      "52                0.9996           0.0004           NaN           NaN   \n",
      "53                1.0000           0.0000           NaN           NaN   \n",
      "54                0.0003           0.9997        1.0000        0.0000   \n",
      "55                0.9934           0.0066           NaN           NaN   \n",
      "56                0.0027           0.9973        0.9870        0.0130   \n",
      "57                0.0020           0.9980        0.0030        0.9970   \n",
      "58                1.0000           0.0000           NaN           NaN   \n",
      "59                0.9975           0.0025           NaN           NaN   \n",
      "60                0.0009           0.9991        0.9998        0.0002   \n",
      "61                1.0000           0.0000           NaN           NaN   \n",
      "62                0.9956           0.0044           NaN           NaN   \n",
      "63                0.9959           0.0041           NaN           NaN   \n",
      "64                0.9997           0.0003           NaN           NaN   \n",
      "65                0.9998           0.0002           NaN           NaN   \n",
      "66                0.9987           0.0013           NaN           NaN   \n",
      "67                0.0013           0.9987        0.0000        1.0000   \n",
      "68                1.0000           0.0000           NaN           NaN   \n",
      "69                1.0000           0.0000           NaN           NaN   \n",
      "70                1.0000           0.0000           NaN           NaN   \n",
      "71                0.0000           1.0000        1.0000        0.0000   \n",
      "72                0.0000           1.0000        0.9968        0.0032   \n",
      "73                0.9998           0.0002           NaN           NaN   \n",
      "74                1.0000           0.0000           NaN           NaN   \n",
      "75                0.9958           0.0042           NaN           NaN   \n",
      "76                0.0000           1.0000        1.0000        0.0000   \n",
      "77                0.9958           0.0042           NaN           NaN   \n",
      "78                0.9963           0.0037           NaN           NaN   \n",
      "79                0.0026           0.9974        1.0000        0.0000   \n",
      "80                0.0035           0.9965        0.0002        0.9998   \n",
      "81                0.9970           0.0030           NaN           NaN   \n",
      "82                0.9945           0.0055           NaN           NaN   \n",
      "83                0.0044           0.9956        0.0162        0.9838   \n",
      "84                0.9980           0.0020           NaN           NaN   \n",
      "85                0.9911           0.0089           NaN           NaN   \n",
      "86                0.9972           0.0028           NaN           NaN   \n",
      "87                0.9973           0.0027           NaN           NaN   \n",
      "88                0.9973           0.0027           NaN           NaN   \n",
      "89                0.9998           0.0002           NaN           NaN   \n",
      "90                0.9998           0.0002           NaN           NaN   \n",
      "91                1.0000           0.0000           NaN           NaN   \n",
      "92                0.9997           0.0003           NaN           NaN   \n",
      "93                0.9995           0.0005           NaN           NaN   \n",
      "94                1.0000           0.0000           NaN           NaN   \n",
      "95                1.0000           0.0000           NaN           NaN   \n",
      "96                0.0008           0.9992        0.9982        0.0018   \n",
      "97                0.9999           0.0001           NaN           NaN   \n",
      "98                0.0011           0.9989        0.9990        0.0010   \n",
      "99                0.0000           1.0000        1.0000        0.0000   \n",
      "100               1.0000           0.0000           NaN           NaN   \n",
      "101               0.9991           0.0009           NaN           NaN   \n",
      "102               0.9998           0.0002           NaN           NaN   \n",
      "103               0.9979           0.0021           NaN           NaN   \n",
      "104               1.0000           0.0000           NaN           NaN   \n",
      "105               0.9996           0.0004           NaN           NaN   \n",
      "106               0.9954           0.0046           NaN           NaN   \n",
      "107               1.0000           0.0000           NaN           NaN   \n",
      "108               0.0011           0.9989        0.0027        0.9973   \n",
      "109               0.9993           0.0007           NaN           NaN   \n",
      "110               0.0037           0.9963        0.9895        0.0105   \n",
      "111               1.0000           0.0000           NaN           NaN   \n",
      "112               0.9986           0.0014           NaN           NaN   \n",
      "113               1.0000           0.0000           NaN           NaN   \n",
      "114               1.0000           0.0000           NaN           NaN   \n",
      "115               0.9987           0.0013           NaN           NaN   \n",
      "116               1.0000           0.0000           NaN           NaN   \n",
      "117               1.0000           0.0000           NaN           NaN   \n",
      "118               0.0010           0.9990        0.9982        0.0018   \n",
      "119               1.0000           0.0000           NaN           NaN   \n",
      "120               0.0050           0.9950        1.0000        0.0000   \n",
      "121               1.0000           0.0000           NaN           NaN   \n",
      "122               0.0018           0.9982        0.0023        0.9977   \n",
      "123               1.0000           0.0000           NaN           NaN   \n",
      "124               1.0000           0.0000           NaN           NaN   \n",
      "125               0.9999           0.0001           NaN           NaN   \n",
      "126               0.9986           0.0014           NaN           NaN   \n",
      "127               0.0002           0.9998        1.0000        0.0000   \n",
      "128               0.0039           0.9961        0.9994        0.0006   \n",
      "129               1.0000           0.0000           NaN           NaN   \n",
      "130               1.0000           0.0000           NaN           NaN   \n",
      "131               0.9999           0.0001           NaN           NaN   \n",
      "132               0.9908           0.0092           NaN           NaN   \n",
      "133               1.0000           0.0000           NaN           NaN   \n",
      "134               0.0023           0.9977        0.0004        0.9996   \n",
      "135               0.0014           0.9986        0.9999        0.0001   \n",
      "136               0.9998           0.0002           NaN           NaN   \n",
      "137               0.0000           1.0000        1.0000        0.0000   \n",
      "138               0.9998           0.0002           NaN           NaN   \n",
      "139               0.9960           0.0040           NaN           NaN   \n",
      "140               0.9999           0.0001           NaN           NaN   \n",
      "141               0.0038           0.9962        0.0021        0.9979   \n",
      "142               0.9973           0.0027           NaN           NaN   \n",
      "143               0.9948           0.0052           NaN           NaN   \n",
      "144               0.9971           0.0029           NaN           NaN   \n",
      "145               0.9999           0.0001           NaN           NaN   \n",
      "146               1.0000           0.0000           NaN           NaN   \n",
      "147               1.0000           0.0000           NaN           NaN   \n",
      "148               0.0038           0.9962        0.9998        0.0002   \n",
      "149               0.0025           0.9975        0.0002        0.9998   \n",
      "150               0.0000           1.0000        0.9891        0.0109   \n",
      "151               0.0071           0.9929        0.0001        0.9999   \n",
      "152               0.9996           0.0004           NaN           NaN   \n",
      "153               1.0000           0.0000           NaN           NaN   \n",
      "154               0.0089           0.9911        0.9959        0.0041   \n",
      "155               0.9999           0.0001           NaN           NaN   \n",
      "156               0.9984           0.0016           NaN           NaN   \n",
      "157               0.3273           0.6727        0.9981        0.0019   \n",
      "158               0.9988           0.0012           NaN           NaN   \n",
      "159               0.9988           0.0012           NaN           NaN   \n",
      "160               1.0000           0.0000           NaN           NaN   \n",
      "161               0.9993           0.0007           NaN           NaN   \n",
      "162               0.0005           0.9995        0.9994        0.0006   \n",
      "163               0.9993           0.0007           NaN           NaN   \n",
      "164               0.9985           0.0015           NaN           NaN   \n",
      "165               0.9994           0.0006           NaN           NaN   \n",
      "166               0.9911           0.0089           NaN           NaN   \n",
      "167               0.9999           0.0001           NaN           NaN   \n",
      "168               0.9985           0.0015           NaN           NaN   \n",
      "169               1.0000           0.0000           NaN           NaN   \n",
      "170               0.9979           0.0021           NaN           NaN   \n",
      "171               0.9927           0.0073           NaN           NaN   \n",
      "\n",
      "    Pred Molec True Molec       Set  \n",
      "0       group3     group4      test  \n",
      "1       group3        shh      test  \n",
      "2       group3     group4      test  \n",
      "3       group3     group4      test  \n",
      "4       group3     group4      test  \n",
      "5       group3     group4      test  \n",
      "6       group3     group4      test  \n",
      "7       group3     group3      test  \n",
      "8          shh     group4      test  \n",
      "9       group3        shh      test  \n",
      "10      group3        wnt      test  \n",
      "11      group3     group4      test  \n",
      "12      group3     group3      test  \n",
      "13      group3     group4      test  \n",
      "14      group3        shh      test  \n",
      "15      group3     group4      test  \n",
      "16      group3        shh      test  \n",
      "17      group3        shh      test  \n",
      "18      group3     group4      test  \n",
      "19      group3     group3      test  \n",
      "20      group3        wnt      test  \n",
      "21         shh     group4      test  \n",
      "22      group3     group4      test  \n",
      "23         shh        shh      test  \n",
      "24      group3        shh      test  \n",
      "25      group3        wnt      test  \n",
      "26      group3     group4      test  \n",
      "27         shh     group4      test  \n",
      "28      group3     group3      test  \n",
      "29         wnt     group4      test  \n",
      "30      group3     group3      test  \n",
      "31         wnt     group4      test  \n",
      "32      group3     group4      test  \n",
      "33      group3     group3      test  \n",
      "34      group3     group4      test  \n",
      "35      group3     group3      test  \n",
      "36      group3     group4      test  \n",
      "37      group3     group3      test  \n",
      "38      group3     group4      test  \n",
      "39      group3     group4      test  \n",
      "40      group3     group3      test  \n",
      "41      group3     group3      test  \n",
      "42      group3     group3      test  \n",
      "43         shh        shh      test  \n",
      "44      group3        wnt      test  \n",
      "45      group3        shh      test  \n",
      "46         shh     group4      test  \n",
      "47         shh        shh  training  \n",
      "48      group3     group4  training  \n",
      "49      group3     group4  training  \n",
      "50      group3     group3  training  \n",
      "51      group3     group4  training  \n",
      "52      group3     group4  training  \n",
      "53      group3     group4  training  \n",
      "54         shh        shh  training  \n",
      "55      group3     group4  training  \n",
      "56         shh        shh  training  \n",
      "57         wnt        wnt  training  \n",
      "58      group3     group3  training  \n",
      "59      group3     group3  training  \n",
      "60         shh        shh  training  \n",
      "61      group3     group4  training  \n",
      "62      group3     group3  training  \n",
      "63      group3     group3  training  \n",
      "64      group3     group4  training  \n",
      "65      group3     group4  training  \n",
      "66      group3     group4  training  \n",
      "67         wnt        wnt  training  \n",
      "68      group3     group4  training  \n",
      "69      group3     group4  training  \n",
      "70      group3     group3  training  \n",
      "71         shh        shh  training  \n",
      "72         shh        shh  training  \n",
      "73      group3     group4  training  \n",
      "74      group3     group4  training  \n",
      "75      group3     group3  training  \n",
      "76         shh        shh  training  \n",
      "77      group3     group4  training  \n",
      "78      group3     group4  training  \n",
      "79         shh        shh  training  \n",
      "80         wnt        wnt  training  \n",
      "81      group3     group3  training  \n",
      "82      group3     group3  training  \n",
      "83         wnt        wnt  training  \n",
      "84      group3     group3  training  \n",
      "85      group3     group3  training  \n",
      "86      group3     group4  training  \n",
      "87      group3     group4  training  \n",
      "88      group3     group4  training  \n",
      "89      group3     group4  training  \n",
      "90      group3     group4  training  \n",
      "91      group3     group4  training  \n",
      "92      group3     group3  training  \n",
      "93      group3     group4  training  \n",
      "94      group3     group4  training  \n",
      "95      group3     group4  training  \n",
      "96         shh        shh  training  \n",
      "97      group3     group4  training  \n",
      "98         shh        shh  training  \n",
      "99         shh        shh  training  \n",
      "100     group3     group3  training  \n",
      "101     group3     group4  training  \n",
      "102     group3     group3  training  \n",
      "103     group3     group4  training  \n",
      "104     group3     group4  training  \n",
      "105     group3     group4  training  \n",
      "106     group3     group4  training  \n",
      "107     group3     group3  training  \n",
      "108        wnt        wnt  training  \n",
      "109     group3     group4  training  \n",
      "110        shh        shh  training  \n",
      "111     group3     group4  training  \n",
      "112     group3     group4  training  \n",
      "113     group3     group3  training  \n",
      "114     group3     group4  training  \n",
      "115     group3     group4  training  \n",
      "116     group3     group4  training  \n",
      "117     group3     group3  training  \n",
      "118        shh        shh  training  \n",
      "119     group3     group4  training  \n",
      "120        shh        shh  training  \n",
      "121     group3     group3  training  \n",
      "122        wnt        wnt  training  \n",
      "123     group3     group3  training  \n",
      "124     group3     group3  training  \n",
      "125     group3     group4  training  \n",
      "126     group3     group4  training  \n",
      "127        shh        shh  training  \n",
      "128        shh        shh  training  \n",
      "129     group3     group3  training  \n",
      "130     group3     group4  training  \n",
      "131     group3     group4  training  \n",
      "132     group3     group4  training  \n",
      "133     group3     group3  training  \n",
      "134        wnt        wnt  training  \n",
      "135        shh        shh  training  \n",
      "136     group3     group4  training  \n",
      "137        shh        shh  training  \n",
      "138     group3     group4  training  \n",
      "139     group3     group4  training  \n",
      "140     group3     group4  training  \n",
      "141        wnt        wnt  training  \n",
      "142     group3     group3  training  \n",
      "143     group3     group3  training  \n",
      "144     group3     group4  training  \n",
      "145     group3     group4  training  \n",
      "146     group3     group4  training  \n",
      "147     group3     group4  training  \n",
      "148        shh        shh  training  \n",
      "149        wnt        wnt  training  \n",
      "150        shh        shh  training  \n",
      "151        wnt        wnt  training  \n",
      "152     group3     group4  training  \n",
      "153     group3     group4  training  \n",
      "154        shh        shh  training  \n",
      "155     group3     group4  training  \n",
      "156     group3     group3  training  \n",
      "157        shh        shh  training  \n",
      "158     group3     group4  training  \n",
      "159     group3     group3  training  \n",
      "160     group3     group4  training  \n",
      "161     group3     group4  training  \n",
      "162        shh        shh  training  \n",
      "163     group3     group4  training  \n",
      "164     group3     group4  training  \n",
      "165     group3     group4  training  \n",
      "166     group3     group3  training  \n",
      "167     group3     group4  training  \n",
      "168     group3     group4  training  \n",
      "169     group3     group4  training  \n",
      "170     group3     group3  training  \n",
      "171     group3     group3  training  \n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_df.to_csv('seq_classification_results.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}