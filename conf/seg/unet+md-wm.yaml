# wm: whole map, attend to the whole feature map in decoder
class_path: mbs.models.MBSegMaskFormerModel
init_args:
  backbone:
    class_path: luolib.models.PlainConvUNetDecoder
    init_args:
      inner:
        class_path: luolib.models.UNetBackbone
        init_args:
          in_channels: 3
          layer_channels: [32, 64, 128, 256, 320, 320]
          kernel_sizes:
          - [1, 3, 3]
          - [1, 3, 3]
          - [1, 3, 3]
          - 3
          - 3
          - 3
          strides:
          - 1
          - [1, 2, 2]
          - [1, 2, 2]
          - 2
          - 2
          - [1, 2, 2]
          res_block: false
      spatial_dims: 3
      layer_channels: ${.inner.init_args.layer_channels}
      kernel_sizes: ${.inner.init_args.kernel_sizes}
      strides: ${.inner.init_args.strides}
  mask_decoder:
    class_path: luolib.models.MaskedAttentionDecoder
    init_args:
      spatial_dims: 3
      feature_channels: 256
      num_attention_heads: 8
      pixel_embedding_dim: 32
      num_decoder_layers: 9
      mask_start_layer: 9
      num_feature_levels: 3
      key_projection_channels: [320, 320, 256]
      num_queries: 2
      grad_ckpt: true
  val_sw: sw.yaml
